"""
Federated Learning with Vision Transformer on CIFAR-100
Extreme Non-IID Scenario: 20 Clients with 1 Superclass Each (Beta â†’ 0)

This implementation simulates extreme data heterogeneity where each of the 20 clients
receives data from exactly one of CIFAR-100's 20 superclasses, representing real-world
scenarios like specialized hospitals, regional preferences, or industry verticals.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR100
from transformers import ViTForImageClassification, ViTImageProcessor
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict, Counter
import copy
import os
import json
from datetime import datetime
from tqdm import tqdm
import warnings
from typing import Dict, List, Tuple, Optional, Any
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# CIFAR-100 Superclass Mapping
CIFAR100_SUPERCLASSES = {
    'aquatic_mammals': [4, 30, 55, 72, 95],
    'fish': [1, 32, 67, 73, 91],
    'flowers': [54, 62, 70, 82, 92],
    'food_containers': [9, 10, 16, 28, 61],
    'fruit_and_vegetables': [0, 51, 53, 57, 83],
    'household_electrical_devices': [22, 39, 40, 86, 87],
    'household_furniture': [5, 20, 25, 84, 94],
    'insects': [6, 7, 14, 18, 24],
    'large_carnivores': [3, 42, 43, 88, 97],
    'large_man-made_outdoor_things': [12, 17, 37, 68, 76],
    'large_natural_outdoor_scenes': [23, 33, 49, 60, 71],
    'large_omnivores_and_herbivores': [15, 19, 21, 31, 38],
    'medium_mammals': [34, 63, 64, 66, 75],
    'non-insect_invertebrates': [26, 45, 77, 79, 99],
    'people': [2, 11, 35, 46, 98],
    'reptiles': [27, 29, 44, 78, 93],
    'small_mammals': [36, 50, 65, 74, 80],
    'trees': [47, 52, 56, 59, 96],
    'vehicles_1': [8, 13, 48, 58, 90],
    'vehicles_2': [41, 69, 81, 85, 89]
}

# ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šåˆ›å»ºå…¨å±€transform pipelineï¼Œé¿å…é‡å¤åˆ›å»º
import torchvision.transforms as transforms

# å…¨å±€transform pipelineï¼Œåªåˆ›å»ºä¸€æ¬¡
GLOBAL_TRANSFORM = transforms.Compose([
    transforms.Resize((224, 224)),  # ViTæ ‡å‡†è¾“å…¥å°ºå¯¸
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNetæ ‡å‡†åŒ–
])

def transform_image(image, processor=None):
    """è½»é‡çº§å›¾åƒé¢„å¤„ç†å‡½æ•°"""
    # ğŸš€ ç›´æ¥ä½¿ç”¨å…¨å±€transformï¼Œå¿½ç•¥processorå‚æ•°
    return GLOBAL_TRANSFORM(image)

class FederatedClient:
    """Federated Learning Client for one superclass"""
    
    def __init__(self, client_id, superclass_name, superclass_classes, device=torch.device('cuda')):
        self.client_id = client_id
        self.superclass_name = superclass_name
        self.superclass_classes = superclass_classes
        self.device = device
        self.model = None
        self.processor = None
        self.train_loader = None
        self.optimizer = None
        
    def setup_model(self, global_model, processor):
        """Initialize client model from global model"""
        # ğŸš€ ä¿®å¤ï¼šåªåœ¨CPUä¸Šä¿å­˜æ¨¡å‹ï¼Œé¿å…20ä¸ªæ¨¡å‹åŒæ—¶å ç”¨æ˜¾å­˜
        self.model = copy.deepcopy(global_model).cpu()  # å¼ºåˆ¶ä¿å­˜åœ¨CPU
        self.processor = processor
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨ä½†ä¸ç«‹å³åˆ›å»ºï¼Œé¿å…é‡å¤åˆ›å»º
        self.optimizer = None
        
    def setup_data(self, dataset, superclass_indices, processor, batch_size=32):
        """Setup client's data loader with pre-computed superclass indices"""
        # Create subset and transformed dataset
        client_subset = Subset(dataset, superclass_indices)
        client_transformed = TransformedCifar100(client_subset, processor)
        
        # ğŸš€ é’ˆå¯¹64GBé…ç½®çš„ä¼˜åŒ–è®¾ç½®ï¼šnum_workers=1 æ€§ä»·æ¯”æœ€é«˜
        # num_workers=0: æœ€ç¨³å®šï¼Œä½†ç¨æ…¢
        # num_workers=1: æ€§ä»·æ¯”æœ€ä½³ï¼Œé€Ÿåº¦æå‡æ˜æ˜¾ï¼Œå†…å­˜å¢é•¿æœ‰é™
        # num_workers>1: å†…å­˜å ç”¨å¤§å¢ï¼Œä½†é€Ÿåº¦æå‡æœ‰é™
        optimized_num_workers = 1  # 64GBé…ç½®ä¸‹çš„æœ€ä½³é€‰æ‹©
        
        self.train_loader = DataLoader(
            client_transformed, 
            batch_size=batch_size,  # ä¿æŒåŸæœ‰batch_sizeï¼Œç¨³å®šä¸ºä¸»
            shuffle=True,
            num_workers=optimized_num_workers,  # è½»åº¦å¹¶è¡Œï¼Œæ€§ä»·æ¯”æœ€é«˜
            pin_memory=True,  # 64GBå†…å­˜å……è¶³ï¼Œå¯ä»¥å¯ç”¨
            persistent_workers=False  # ğŸš€ ä¿®å¤ï¼šç¦ç”¨persistent_workersï¼Œé¿å…å†…å­˜ç´¯ç§¯
        )
        
        print(f"Client {self.client_id} ({self.superclass_name}): {len(superclass_indices)} samples")
        print(f"  Memory-Safe Config: batch_size={batch_size}, num_workers={optimized_num_workers}, persistent_workers=False")
        
    def local_train(self, epochs=1, lr=2e-5):
        """Perform local training"""
        print(f"    Training Client {self.client_id} ({self.superclass_name})...")
        
        # ç±»å‹æ£€æŸ¥ï¼šç¡®ä¿train_loaderå·²åˆå§‹åŒ–
        if self.train_loader is None:
            raise ValueError(f"Client {self.client_id}: train_loader not initialized")
        
        # ğŸš€ ä¿®å¤ï¼šè®­ç»ƒå‰å°†æ¨¡å‹ç§»åˆ°GPU
        self.model = self.model.to(self.device)  # type: ignore
        self.model.train()
        
        # åªåœ¨ç¬¬ä¸€æ¬¡æˆ–éœ€è¦æ—¶åˆ›å»ºä¼˜åŒ–å™¨
        if self.optimizer is None:
            if self.model is None:
                raise ValueError(f"Client {self.client_id}: model not initialized")
            self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)
        
        total_loss = 0
        total_samples = 0
        
        for epoch in range(epochs):
            epoch_loss = 0
            # æ·»åŠ æ‰¹æ¬¡çº§åˆ«çš„è¿›åº¦æ¡
            batch_pbar = tqdm(self.train_loader, desc=f"      Epoch {epoch+1}/{epochs}", leave=False)
            
            for batch_idx, batch in enumerate(batch_pbar):
                pixel_values = batch['pixel_values'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                self.optimizer.zero_grad()
                outputs = self.model(pixel_values=pixel_values, labels=labels)
                loss = outputs.loss
                loss.backward()
                self.optimizer.step()
                
                epoch_loss += loss.item()
                total_samples += labels.size(0)
                
                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰loss
                batch_pbar.set_postfix({'loss': f'{loss.item():.4f}'})
                
            total_loss += epoch_loss
        
        # ğŸš€ ä¿®å¤ï¼šè®­ç»ƒå®Œæˆåç«‹å³å°†æ¨¡å‹ç§»å›CPUï¼Œé‡Šæ”¾GPUæ˜¾å­˜
        self.model = self.model.cpu()
        torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
        
        # ğŸš€ æ–°å¢ï¼šå¼ºåˆ¶å†…å­˜æ¸…ç†ï¼Œè§£å†³ç´¯ç§¯é—®é¢˜
        import gc
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
        
        # æ¸…ç†å¯èƒ½çš„ä¸´æ—¶å˜é‡
        del batch_pbar
        if torch.cuda.is_available():
            torch.cuda.synchronize()  # ç­‰å¾…GPUæ“ä½œå®Œæˆ
            torch.cuda.empty_cache()  # å†æ¬¡æ¸…ç†GPUç¼“å­˜
            
        avg_loss = total_loss / (epochs * len(self.train_loader))
        print(f"    Client {self.client_id} finished: avg_loss={avg_loss:.4f}")
        return avg_loss, total_samples
    
    def get_model_parameters(self):
        """Get model parameters for aggregation"""
        if self.model is None:
            raise ValueError(f"Client {self.client_id}: model not initialized")
        return {name: param.cpu().clone() for name, param in self.model.named_parameters()}
    
    def cleanup_resources(self):
        """æ¸…ç†å®¢æˆ·ç«¯èµ„æºï¼Œé˜²æ­¢å†…å­˜æ³„æ¼"""
        # ğŸš€ ä¿®å¤ï¼šä¸å®Œå…¨åˆ é™¤train_loaderï¼Œåªæ¸…ç†å†…éƒ¨èµ„æº
        # æ¸…ç†DataLoaderçš„å†…éƒ¨ç¼“å­˜å’Œè¿­ä»£å™¨ï¼Œä½†ä¿ç•™DataLoaderå¯¹è±¡
        if hasattr(self, 'train_loader') and self.train_loader is not None:
            try:
                # æ¸…ç†å¯èƒ½å­˜åœ¨çš„è¿­ä»£å™¨
                if hasattr(self.train_loader, '_iterator'):
                    del self.train_loader._iterator
                # ä¸åˆ é™¤train_loaderæœ¬èº«ï¼Œå› ä¸ºåç»­è½®æ¬¡è¿˜éœ€è¦ä½¿ç”¨
            except:
                pass
        
        # æ¸…ç†ä¼˜åŒ–å™¨ï¼ˆä¸‹æ¬¡è®­ç»ƒæ—¶ä¼šé‡æ–°åˆ›å»ºï¼‰
        if hasattr(self, 'optimizer') and self.optimizer is not None:
            del self.optimizer
            self.optimizer = None
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

class FederatedServer:
    """Federated Learning Server implementing FedAvg"""
    
    def __init__(self, device=torch.device('cuda')):
        self.device = device
        self.global_model = None
        self.processor = None
        self.clients = []
        self.test_loader = None
        
        # Tracking metrics
        self.round_history = {
            'round': [],
            'global_accuracy': [],
            'superclass_accuracies': [],
            'avg_client_loss': [],
            'num_participating_clients': []
        }
        
    def setup_global_model(self):
        """Initialize global ViT model"""
        try:
            self.processor = ViTImageProcessor.from_pretrained(
                'google/vit-base-patch16-224',
                do_resize=True,
                size=224,
                do_rescale=False  # å›¾åƒå·²ç»åœ¨[0,1]èŒƒå›´ï¼Œä¸éœ€è¦é‡æ–°rescale
            )
            
            self.global_model = ViTForImageClassification.from_pretrained(
                'google/vit-base-patch16-224',
                num_labels=100,  # CIFAR-100 has 100 classes
                ignore_mismatched_sizes=True
            )
            if isinstance(self.device, str):
                self.global_model = self.global_model.to(torch.device(self.device))  # type: ignore
            else:
                self.global_model = self.global_model.to(self.device)  # type: ignore
            
            print("Global ViT model initialized for CIFAR-100")
        except Exception as e:
            raise RuntimeError(f"Failed to initialize global model: {e}")
            
        # ğŸš€ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åˆå§‹åŒ–æˆåŠŸ
        if self.global_model is None or self.processor is None:
            raise RuntimeError("Global model or processor initialization failed")
        
    def setup_test_data(self, batch_size=64):
        """Setup global test set"""
        test_dataset = CIFAR100(root='./data', train=False, download=True)
        test_transformed = TransformedCifar100(test_dataset, self.processor)
        
        # ğŸš€ æµ‹è¯•æ—¶å¯ä»¥ç¨å¤§batch_sizeï¼Œå› ä¸ºæ— éœ€æ¢¯åº¦è®¡ç®—
        # ä½†ä»ç„¶ä¿å®ˆï¼Œé¿å…æ˜¾å­˜é—®é¢˜
        safe_batch_size = 64  # æµ‹è¯•æ—¶é€‚ä¸­çš„batch_size
        
        self.test_loader = DataLoader(
            test_transformed, 
            batch_size=safe_batch_size, 
            shuffle=False, 
            num_workers=1,  # æµ‹è¯•æ—¶ä¹Ÿä½¿ç”¨è½»åº¦å¹¶è¡Œ
            pin_memory=True,
            persistent_workers=False  # ğŸš€ ä¿®å¤ï¼šæµ‹è¯•æ—¶ä¹Ÿç¦ç”¨persistent_workers
        )
        print(f"Test set: {len(test_dataset)} samples (batch_size={safe_batch_size})")
        
    def register_client(self, client):
        """Register a client"""
        self.clients.append(client)
        
    def distribute_model(self):
        """Send global model to all clients with memory optimization"""
        for client in self.clients:
            # ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šå…ˆæ¸…ç†æ—§æ¨¡å‹å†åˆ†å‘æ–°æ¨¡å‹
            if hasattr(client, 'model') and client.model is not None:
                del client.model
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            client.setup_model(self.global_model, self.processor)
            
    def aggregate_models(self, client_updates):
        """Aggregate client model updates using FedAvg with memory optimization"""
        if self.global_model is None:
            raise ValueError("Global model not initialized")
            
        # ğŸš€ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æœ‰å®¢æˆ·ç«¯æ›´æ–°
        if not client_updates:
            raise ValueError("No client updates received for aggregation")
            
        # Calculate total samples for weighted averaging
        total_samples = sum(samples for _, samples in client_updates)
        
        # ğŸš€ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æœ‰è®­ç»ƒæ ·æœ¬
        if total_samples == 0:
            raise ValueError("Total training samples is zero - all clients failed")
        
        # Initialize aggregated parameters
        aggregated_params = {}
        
        # ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šæµå¼å¤„ç†å®¢æˆ·ç«¯æ›´æ–°ï¼Œå‡å°‘å³°å€¼å†…å­˜
        for i, (client_params, num_samples) in enumerate(client_updates):
            weight = num_samples / total_samples
            
            for name, param in client_params.items():
                if name not in aggregated_params:
                    aggregated_params[name] = weight * param.clone()
                else:
                    aggregated_params[name] += weight * param
            
            # ç«‹å³é‡Šæ”¾å·²å¤„ç†çš„å®¢æˆ·ç«¯å‚æ•°ï¼Œå‡å°‘å†…å­˜ç´¯ç§¯
            del client_params
            
            # æ¯å¤„ç†5ä¸ªå®¢æˆ·ç«¯å°±æ¸…ç†ä¸€æ¬¡å†…å­˜
            if (i + 1) % 5 == 0:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        # Update global model
        global_dict = self.global_model.state_dict()
        for name, param in aggregated_params.items():
            if name in global_dict:
                global_dict[name] = param
        
        self.global_model.load_state_dict(global_dict)
        
        # æ¸…ç†èšåˆå‚æ•°
        del aggregated_params
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
    def evaluate_global_model(self):
        """Evaluate global model on test set"""
        if self.global_model is None:
            raise ValueError("Global model not initialized")
        if self.test_loader is None:
            raise ValueError("Test loader not initialized")
            
        self.global_model.eval()
        correct = 0
        total = 0
        superclass_correct = defaultdict(int)
        superclass_total = defaultdict(int)
        
        with torch.no_grad():
            for batch in tqdm(self.test_loader, desc="Evaluating"):
                pixel_values = batch['pixel_values'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.global_model(pixel_values=pixel_values)
                _, predicted = torch.max(outputs.logits, 1)
                
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                # Track superclass performance
                for i, label in enumerate(labels):
                    for superclass_name, classes in CIFAR100_SUPERCLASSES.items():
                        if label.item() in classes:
                            superclass_total[superclass_name] += 1
                            if predicted[i] == label:
                                superclass_correct[superclass_name] += 1
                            break
        
        global_accuracy = 100 * correct / total
        
        # Calculate superclass accuracies
        superclass_accuracies = {}
        for superclass_name in CIFAR100_SUPERCLASSES.keys():
            if superclass_total[superclass_name] > 0:
                acc = 100 * superclass_correct[superclass_name] / superclass_total[superclass_name]
                superclass_accuracies[superclass_name] = acc
            else:
                superclass_accuracies[superclass_name] = 0.0
                
        return global_accuracy, superclass_accuracies
    
    def federated_round(self, round_num, local_epochs=1):
        """Execute one round of federated learning"""
        print(f"\n=== Federated Round {round_num} ===")
        
        # Distribute global model to clients
        self.distribute_model()
        
        # Client local training
        client_updates = []
        total_loss = 0
        failed_clients = 0
        
        for client in tqdm(self.clients, desc="Client Training"):
            try:
                loss, num_samples = client.local_train(epochs=local_epochs)
                client_params = client.get_model_parameters()
                client_updates.append((client_params, num_samples))
                total_loss += loss
                
                # ğŸš€ æ–°å¢ï¼šæ¯ä¸ªå®¢æˆ·ç«¯è®­ç»ƒå®Œæˆåç«‹å³æ¸…ç†èµ„æº
                client.cleanup_resources()
            except Exception as e:
                print(f"âš ï¸  Client {client.client_id} failed: {e}")
                failed_clients += 1
                # ç»§ç»­è®­ç»ƒå…¶ä»–å®¢æˆ·ç«¯ï¼Œä¸ä¸­æ–­æ•´ä¸ªè¿‡ç¨‹
                continue
        
        # ğŸš€ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è‡³å°‘æœ‰ä¸€äº›å®¢æˆ·ç«¯æˆåŠŸ
        if len(client_updates) == 0:
            raise RuntimeError(f"All {len(self.clients)} clients failed in round {round_num}")
        
        if failed_clients > 0:
            print(f"âš ï¸  {failed_clients} clients failed, continuing with {len(client_updates)} successful clients")
            
        avg_client_loss = total_loss / len(client_updates)  # ä½¿ç”¨æˆåŠŸçš„å®¢æˆ·ç«¯æ•°é‡
        
        # Server aggregation
        self.aggregate_models(client_updates)
        
        # Global evaluation
        global_accuracy, superclass_accuracies = self.evaluate_global_model()
        
        # Record metrics
        self.round_history['round'].append(round_num)
        self.round_history['global_accuracy'].append(global_accuracy)
        self.round_history['superclass_accuracies'].append(superclass_accuracies)
        self.round_history['avg_client_loss'].append(avg_client_loss)
        self.round_history['num_participating_clients'].append(len(self.clients))
        
        print(f"Round {round_num} Results:")
        print(f"  Global Accuracy: {global_accuracy:.2f}%")
        print(f"  Average Client Loss: {avg_client_loss:.4f}")
        print(f"  Participating Clients: {len(self.clients)}")
        
        return global_accuracy, superclass_accuracies, avg_client_loss

class TransformedCifar100(torch.utils.data.Dataset):
    """é¢„å¤„ç†åçš„CIFAR100æ•°æ®é›†ï¼Œé¿å…è®­ç»ƒæ—¶é‡å¤å¤„ç†"""
    def __init__(self, dataset, processor):
        self.dataset = dataset
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„å…ˆå­˜å‚¨å¤„ç†å™¨å‚æ•°ï¼Œé¿å…é‡å¤è°ƒç”¨
        self.processor = processor

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        if hasattr(self.dataset, 'indices'):  # Subset
            image, label = self.dataset.dataset[self.dataset.indices[idx]]
        else:
            image, label = self.dataset[idx]
        
        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨è½»é‡çº§transformæ›¿ä»£é‡å¤çš„processorè°ƒç”¨
        pixel_values = transform_image(image, self.processor)
        return {"pixel_values": pixel_values, "labels": torch.tensor(label)}

def setup_federated_data():
    """Setup federated data distribution with optimized indexing"""
    # åŸºç¡€æ•°æ®é›†ï¼Œä¸åšé¢„å¤„ç†
    train_dataset = CIFAR100(root='./data', train=True, download=True)
    test_dataset = CIFAR100(root='./data', train=False, download=True)
    print(f"Total training samples: {len(train_dataset)}")
    
    # ğŸš€ é¢„å…ˆæ„å»ºè¶…ç±»åˆ°æ ·æœ¬ç´¢å¼•çš„æ˜ å°„ï¼Œåªéå†ä¸€æ¬¡ï¼
    print("Building superclass index mapping...")
    superclass_to_indices = {name: [] for name in CIFAR100_SUPERCLASSES.keys()}
    
    for idx, (_, label) in enumerate(train_dataset):  # type: ignore
        for superclass_name, classes in CIFAR100_SUPERCLASSES.items():
            if label in classes:
                superclass_to_indices[superclass_name].append(idx)
                break
    
    # éªŒè¯æ•°æ®åˆ†å¸ƒ
    for name, indices in superclass_to_indices.items():
        print(f"Superclass {name}: {len(indices)} samples")
    
    return train_dataset, superclass_to_indices

def create_visualizations(server, save_dir):
    """Create comprehensive visualizations"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Federated Learning Results: 20 Clients, 1 Superclass Each (Î²â†’0)', fontsize=16)
    
    # 1. Global Accuracy Over Rounds
    axes[0, 0].plot(server.round_history['round'], server.round_history['global_accuracy'], 
                   'b-o', linewidth=2, markersize=6)
    axes[0, 0].set_title('Global Test Accuracy')
    axes[0, 0].set_xlabel('Federated Round')
    axes[0, 0].set_ylabel('Accuracy (%)')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Average Client Loss
    axes[0, 1].plot(server.round_history['round'], server.round_history['avg_client_loss'], 
                   'r-s', linewidth=2, markersize=6)
    axes[0, 1].set_title('Average Client Training Loss')
    axes[0, 1].set_xlabel('Federated Round')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Final Superclass Performance Heatmap
    if server.round_history['superclass_accuracies']:
        final_superclass_acc = server.round_history['superclass_accuracies'][-1]
        superclass_names = list(final_superclass_acc.keys())
        accuracies = list(final_superclass_acc.values())
        
        # Create heatmap data
        acc_matrix = np.array(accuracies).reshape(1, -1)
        im = axes[0, 2].imshow(acc_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)
        axes[0, 2].set_title('Final Superclass Accuracies')
        axes[0, 2].set_xticks(range(len(superclass_names)))
        axes[0, 2].set_xticklabels([name[:10] + '...' if len(name) > 10 else name 
                                  for name in superclass_names], rotation=45, ha='right')
        axes[0, 2].set_yticks([])
        
        # Add colorbar
        cbar = plt.colorbar(im, ax=axes[0, 2])
        cbar.set_label('Accuracy (%)')
    
    # 4. Superclass Performance Evolution
    if len(server.round_history['superclass_accuracies']) > 1:
        selected_superclasses = ['vehicles_1', 'people', 'flowers', 'large_carnivores', 'fish']
        for superclass in selected_superclasses:
            if superclass in CIFAR100_SUPERCLASSES:
                accs = [round_acc.get(superclass, 0) for round_acc in server.round_history['superclass_accuracies']]
                axes[1, 0].plot(server.round_history['round'], accs, 
                              'o-', linewidth=2, label=superclass, markersize=4)
        
        axes[1, 0].set_title('Selected Superclass Performance Evolution')
        axes[1, 0].set_xlabel('Federated Round')
        axes[1, 0].set_ylabel('Accuracy (%)')
        axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        axes[1, 0].grid(True, alpha=0.3)
    
    # 5. Data Distribution Visualization
    superclass_sizes = [len(classes) for classes in CIFAR100_SUPERCLASSES.values()]
    axes[1, 1].bar(range(len(CIFAR100_SUPERCLASSES)), superclass_sizes)
    axes[1, 1].set_title('Classes per Superclass (All Equal: 5)')
    axes[1, 1].set_xlabel('Superclass Index')
    axes[1, 1].set_ylabel('Number of Classes')
    axes[1, 1].grid(True, alpha=0.3)
    
    # 6. Summary Statistics
    if server.round_history['global_accuracy']:
        final_acc = server.round_history['global_accuracy'][-1]
        max_acc = max(server.round_history['global_accuracy'])
        min_acc = min(server.round_history['global_accuracy'])
        
        summary_text = f"""
Final Results Summary:
â€¢ Total Rounds: {len(server.round_history['round'])}
â€¢ Final Global Accuracy: {final_acc:.2f}%
â€¢ Best Global Accuracy: {max_acc:.2f}%
â€¢ Worst Global Accuracy: {min_acc:.2f}%
â€¢ Number of Clients: 20
â€¢ Data Distribution: Extreme Non-IID
â€¢ Each Client: 1 Superclass (5 classes)
â€¢ Algorithm: FedAvg
â€¢ Model: ViT-base-patch16-224
        """
        
        axes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes, 
                       fontsize=10, verticalalignment='top', 
                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        axes[1, 2].set_xlim(0, 1)
        axes[1, 2].set_ylim(0, 1)
        axes[1, 2].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'federated_learning_results.png'), 
                dpi=300, bbox_inches='tight')
    plt.show()

def save_results(server, save_dir):
    """Save all results and models"""
    os.makedirs(save_dir, exist_ok=True)
    
    # Save global model
    torch.save(server.global_model.state_dict(), 
               os.path.join(save_dir, 'global_model.pth'))
    server.processor.save_pretrained(os.path.join(save_dir, 'processor'))
    
    # Save training history
    with open(os.path.join(save_dir, 'training_history.json'), 'w') as f:
        # Convert numpy types to native Python types for JSON serialization
        history_json = {}
        for key, value in server.round_history.items():
            if key == 'superclass_accuracies':
                history_json[key] = value  # List of dicts
            else:
                history_json[key] = [float(v) if isinstance(v, (np.floating, np.integer)) else v for v in value]
        json.dump(history_json, f, indent=2)
    
    # Save experiment metadata
    metadata = {
        'experiment_type': 'Federated Learning - Extreme Non-IID (Beta â†’ 0)',
        'model': 'ViT-base-patch16-224',
        'dataset': 'CIFAR-100',
        'num_clients': len(server.clients),
        'data_distribution': 'Each client gets 1 superclass (5 classes)',
        'algorithm': 'FedAvg',
        'num_rounds': len(server.round_history['round']),
        'final_global_accuracy': float(server.round_history['global_accuracy'][-1]) if server.round_history['global_accuracy'] else None,
        'timestamp': datetime.now().isoformat(),
        'superclass_mapping': CIFAR100_SUPERCLASSES
    }
    
    with open(os.path.join(save_dir, 'experiment_metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nResults saved to: {save_dir}")
    print(f"- Global model: global_model.pth")
    print(f"- Processor: processor/")
    print(f"- Training history: training_history.json")
    print(f"- Metadata: experiment_metadata.json")
    print(f"- Visualizations: federated_learning_results.png")

def main(resume_from_checkpoint=None):
    """Main federated learning execution"""
    print("=" * 60)
    print("Federated Learning with ViT on CIFAR-100")
    print("Extreme Non-IID: 20 Clients, 1 Superclass Each (Î²â†’0)")
    if resume_from_checkpoint:
        print(f"ğŸ“‚ Resuming from checkpoint: {resume_from_checkpoint}")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # ğŸš€ æ·»åŠ GPUæ£€æŸ¥
    if torch.cuda.is_available():
        print(f"GPU Name: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"Current GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB allocated")
    else:
        print("âš ï¸  CUDA not available, will use CPU (very slow)")
    
    # Initialize server
    server = FederatedServer(device=device)
    server.setup_global_model()
    server.setup_test_data()
    
    # ğŸš€ ä»checkpointæ¢å¤è®­ç»ƒçŠ¶æ€
    start_round = 1
    if resume_from_checkpoint:
        try:
            print(f"ğŸ“‚ Loading checkpoint from {resume_from_checkpoint}")
            
            # è°ƒè¯•ï¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            model_path = os.path.join(resume_from_checkpoint, 'global_model.pth')
            history_path = os.path.join(resume_from_checkpoint, 'training_history.json')
            print(f"ğŸ” Model file exists: {os.path.exists(model_path)}")
            print(f"ğŸ” History file exists: {os.path.exists(history_path)}")
            
            # åŠ è½½è®­ç»ƒå†å²ï¼ˆå…ˆåŠ è½½å†å²å†åŠ è½½æ¨¡å‹ï¼‰
            if os.path.exists(history_path):
                with open(history_path, 'r') as f:
                    history = json.load(f)
                server.round_history = history
                start_round = len(history['round']) + 1
                print(f"âœ… Training history loaded, resuming from round {start_round}")
                print(f"   Previous best accuracy: {max(history['global_accuracy']):.2f}%")
            else:
                print("âŒ Training history file not found!")
            
            # åŠ è½½æ¨¡å‹æƒé‡
            if os.path.exists(model_path) and server.global_model is not None:
                state_dict = torch.load(model_path, map_location=device)
                server.global_model.load_state_dict(state_dict)
                print("âœ… Global model loaded successfully")
            else:
                print("âŒ Model file not found or server.global_model is None!")
            
        except Exception as e:
            print(f"âŒ Failed to load checkpoint: {e}")
            import traceback
            traceback.print_exc()
            print("Starting fresh training...")
            start_round = 1
    
    # Setup federated data with optimized indexing
    train_dataset, superclass_to_indices = setup_federated_data()
    
    # Create 20 clients, one for each superclass
    print("\nCreating 20 federated clients...")
    for client_id, (superclass_name, superclass_classes) in enumerate(CIFAR100_SUPERCLASSES.items()):
        client = FederatedClient(
            client_id=client_id,
            superclass_name=superclass_name,
            superclass_classes=superclass_classes,
            device=device
        )
        
        # Setup client's data using pre-computed indices
        client.setup_data(train_dataset, superclass_to_indices[superclass_name], server.processor, batch_size=32)
        server.register_client(client)
    
    print(f"\nFederated setup complete:")
    print(f"- Server: 1 global ViT model")
    print(f"- Clients: {len(server.clients)}")
    print(f"- Data distribution: Extreme Non-IID (1 superclass per client)")
    
    # Run federated learning
    num_rounds = 25  # ğŸš€ çœ‹çœ‹ViTçš„çœŸå®ä¸Šé™ï¼
    local_epochs = 3
    
    print(f"\nStarting federated learning...")
    print(f"- Rounds: {num_rounds}")
    print(f"- Local epochs per round: {local_epochs}")
    
    # ğŸš€ ç§»é™¤Ctrl+Cä¸­æ–­æœºåˆ¶ï¼Œé¿å…è¯¯è§¦å‘
    try:
        for round_num in range(start_round, num_rounds + 1):
            print(f"\nğŸš€ Starting Round {round_num}/{num_rounds}")
            global_acc, superclass_accs, avg_loss = server.federated_round(
                round_num, local_epochs=local_epochs
            )
            
            # Print top and bottom performing superclasses
            sorted_superclasses = sorted(superclass_accs.items(), key=lambda x: x[1], reverse=True)
            print(f"  Top 3 superclasses: {sorted_superclasses[:3]}")
            print(f"  Bottom 3 superclasses: {sorted_superclasses[-3:]}")
            
            # ğŸš€ æ¯è½®åä¿å­˜ä¸­é—´ç»“æœï¼Œé˜²æ­¢é•¿æ—¶é—´è®­ç»ƒä¸¢å¤±
            if round_num % 2 == 0 or round_num == num_rounds:  # æ¯2è½®æˆ–æœ€åä¸€è½®ä¿å­˜
                temp_save_dir = f"federated_results_round_{round_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                try:
                    save_results(server, temp_save_dir)
                    print(f"  âœ… Intermediate results saved to: {temp_save_dir}")
                except Exception as e:
                    print(f"  âš ï¸ Failed to save intermediate results: {e}")
                    
    except Exception as e:
        print(f"\nâŒ Training failed at round {round_num}: {e}")
        print("Saving progress before exit...")
        error_save_dir = f"federated_results_error_round_{round_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        try:
            save_results(server, error_save_dir)
            print(f"Progress saved to: {error_save_dir}")
        except:
            print("âŒ Failed to save progress")
        raise  # é‡æ–°æŠ›å‡ºé”™è¯¯ä»¥ä¾¿è°ƒè¯•
    
    # Final evaluation and visualization
    print(f"\nFederated learning completed!")
    final_global_acc = server.round_history['global_accuracy'][-1]
    print(f"Final global accuracy: {final_global_acc:.2f}%")
    
    # Save results
    save_dir = f"federated_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    save_results(server, save_dir)
    
    # Create visualizations
    create_visualizations(server, save_dir)
    
    print("\nExperiment completed successfully!")
    return server

if __name__ == "__main__":
    # ğŸš€ ä½¿ç”¨ç¤ºä¾‹ï¼š
    # ä»å¤´å¼€å§‹è®­ç»ƒï¼š
    # server = main()
    
    # ä»checkpointç»§ç»­è®­ç»ƒï¼ˆæŠŠè·¯å¾„æ”¹æˆä½ çš„checkpointæ–‡ä»¶å¤¹ï¼‰ï¼š
    # server = main(resume_from_checkpoint="federated_results_round_10_20250617_XXXXXX")
    
    # ğŸš€ ä»ç¬¬8è½®ç»§ç»­è®­ç»ƒåˆ°ç¬¬20è½®
    server = main(resume_from_checkpoint="federated_results_round_8_20250617_224342")
